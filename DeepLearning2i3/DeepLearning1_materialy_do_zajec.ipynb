{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metoda spadku gradientu (gradient descent)\n",
    "\n",
    "Mamy problem optymalizacyjny: szukamy parametru $\\theta$, dla którego funkcja $f(\\theta)$ przyjmuje wartość najmniejszą.\n",
    "\n",
    "Algorytm:\n",
    "\n",
    "Iteracyjnie poprawiamy wartość parametru według wzoru:\n",
    "\n",
    "$$\\theta_{new} = \\theta_{old} - learning\\_rate * \\frac{df}{d\\theta}$$\n",
    "\n",
    "Dlaczego tak?\n",
    "- gdy funkcja dla danego $\\theta$ jest rosnąca, to pochodna jest dodatnia, więc przesumamy się w lewo,\n",
    "- gdy funkcja dla danego $\\theta$ jest malejąca, to pochodna jest ujemna, więc przesumamy się w prawo.\n",
    "\n",
    "W skrócie: sprawdzamy w którą stronę funkcja maleje i tam się przesywamy - tym dalej im nachylenie większe.\n",
    "\n",
    "<img src=\"Grafika/gradient_descent1.png\" width=\"400\">\n",
    "\n",
    "Źródło: https://hackernoon.com/gradient-descent-aynk-7cbe95a778da\n",
    "\n",
    "\n",
    "W przypadku funkcji wielu zmiennych np. dwóch - $f(x_1, x_2)$:\n",
    "\n",
    "$$\\frac{df}{dx} = grad(f) = \\big[\\frac{df}{dx_1}, \\frac{df}{dx_2}\\big].$$\n",
    "\n",
    "<img src=\"Grafika/gradient_descent2.png\" width=\"400\">\n",
    "\n",
    "Źródło: https://hackernoon.com/gradient-descent-aynk-7cbe95a778da\n",
    "\n",
    "<img src=\"Grafika/gradient_descent3.png\" width=\"400\">\n",
    "\n",
    "Źródło: https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie odbywa się w \"epokach\" - jedna epoka to aktualizaja wartosci parametru na podstawie całego zbioru obserwacji.\n",
    "\n",
    "## Gradient descent:\n",
    "$$ Cost(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^n f(x_i,y_i,\\theta)$$\n",
    "$$\\theta_k = \\theta_k - learning\\_rate * \\frac{dCost}{d\\theta_k}$$\n",
    "\n",
    "## Stochastic gradient decsent - SGD:\n",
    "\n",
    "\n",
    "$$ Cost(\\theta) = \\frac{1}{r} \\sum\\limits_{i \\in \\{ i_1, ..., i_r \\}} f(x_i,y_i,\\theta),$$ $$ \\ \\ \\text{gdzie} \\ \\ \\{ i_1, ..., i_r \\} - \\text{losowy podzbiór obserwacji}$$\n",
    "$$\\theta_i = \\theta_i - learning\\_rate * \\frac{dCost}{d\\theta_i}$$\n",
    "\n",
    "w skrócie, jeśli $$\\theta = \\theta - learning\\_rate \\cdot grad(Cost),$$\n",
    "\n",
    "gdzie \"grad\" oznacza gradient: $grad(f) = [\\frac{df}{d\\theta_1}, \\frac{df}{d\\theta_2}, \\ldots, \\frac{df}{d\\theta_k}]$.\n",
    "\n",
    "\n",
    "Powtarzamy to wielokrotnie tak, żeby każda obserwacja została wykorzystana jeden raz - w praktyce mieszamy losowo kolejność obserwacji i bierzemy kolejne podzbiory - np. dla \"batcha\" wielkości 10, uczymy kolejno na obserwacjach od 1 do 10, od 11 do 20, itd.. Przejście po całych danych to jedna *epoka*.\n",
    "\n",
    "SGD jest fundamentalnym algorytmem uczenia sieci neuronowych wszelkiego rodzaju. (W praktyce stosowane są różne jego modyfikacje)\n",
    "\n",
    "<img src=\"Grafika/stochastic_gradient_descent.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Żródło: Melki, Gabriella. (2016). Fast Online Training of L1 Support Vector Machines. 10.13140/RG.2.1.1944.4087. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron\n",
    "\n",
    "<img src=\"Grafika/neuron.jpg\" width=\"500\">\n",
    "\n",
    "Źródło: https://cdn-images-1.medium.com/max/1600/0*l4ohhbrwQ5MGvmGc.jpg\n",
    "\n",
    "<br>\n",
    "\n",
    "## Matematyczny model neuronu\n",
    "\n",
    "\n",
    "<img src=\"Grafika/perceptron.gif\" width=\"400\">\n",
    "Źródło: http://blog.zabarauskas.com/img/perceptron.gif\n",
    "\n",
    "$\\sigma(\\cdot)$ - funkcja aktywacji\n",
    "\n",
    "## $\\sigma(x) = \\frac{1}{1+\\exp{(-x)}}$\n",
    "\n",
    "W praktyce popularne są trzy funkcje aktywacji:\n",
    "\n",
    "- sigmoid\n",
    "- tangens hiperboliczny\n",
    "- RELU: $relu(x) = \\max{(x,0)}$.\n",
    "\n",
    "### Neuron zastosowany jako klasyfikator:  Perceptron\n",
    "\n",
    "Dla sigmoidalnej funkcji aktywacji perceptron to po prosu regresja logistyczna, tylko uczona metodą spadku gradientu.\n",
    "\n",
    "W sklearn: `sklearn.linear_model.SGDClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wielowarstwowa sieć neuronowa\n",
    "\n",
    "(*Multilayer perceptron*, *feedforward neural network*)\n",
    "\n",
    "\n",
    "<img src=\"Grafika/MLP.jpg\" width=\"700\">\n",
    "Źródło: https://www.intechopen.com/source/html/39071/media/f2.jpg\n",
    "\n",
    "\n",
    "**Uwaga:** \"Input layer\" pomimo tego, że ma w nazwie słowo \"warstwa\", to tak naprawdę to nie jest żadna warstwa sieci... To są po prostu dane wejściowe... Przyjęło się~literaturze\n",
    "nazywanie tego w ten sposób, co może być mylące.\n",
    "\n",
    "\n",
    "Sieci uczy się poprzez minimalizację funkcji kosztu (entropię w przypadku klasyfikacji - to samo co w regresji logistycznej; błędu średniokwadratowego w przypadku regresji; (można używać również innych funkcji)) metodą spadku gradientu (pewnymi wariantami tej metody). Uczenie wykorzystuje algorytm **propagacji wsteczej** (https://en.wikipedia.org/wiki/Backpropagation).\n",
    "\n",
    "**Uwaga!** Sieci neuronowe absolutnie zawsze wymagają zestandaryzowanych danych! Niezależnie od tego czy wykorzystujemy regularyzację czy nie i niezależnie od typu sieci!\n",
    "\n",
    "\n",
    "## Wizualizacja obszarów decyzyjnych w zależności od liczby neuronów\n",
    "\n",
    "### (sieć jednowarstwowa)\n",
    "\n",
    "<img src=\"Grafika/nn-from-scratch-hidden-layer-varying-655x1024.png\" width=\"700\">\n",
    "Źródło: http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/nn-from-scratch-hidden-layer-varying-655x1024.png\n",
    "\n",
    "\n",
    "### Fakt matematyczny: jednowarstwową siecią możemy otrzymać dowolny kształt. \n",
    "\n",
    "Co z tego wynika? To, że (teoretycznie) zawsze wystarczy sieć jednowarstwowa (odpowiednio duża). W praktyce rzeczywiście z reguły wystarcza jedna warstwa, ale mimo wszystko zawsze warto sprawdzić czy 2 (lub 3) nie zadziałają przypadkiem lepiej. Przy czym jeżeli dla dwóch warstw jest gorzej, to nie ma sensu sprawdzać dla większej liczby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieć neuronowa w sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.77%\n"
     ]
    }
   ],
   "source": [
    "dataset = np.loadtxt('Dane/pima-indians-diabetes.data', delimiter=\",\")\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier((20,10))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - biblioteka do pracy z sieciami neuronowymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konstrukcja sieci neuronowej mającej trzy warstwy o liczbach neuronów 100,50,10 i funkcji aktywacji sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,471\n",
      "Trainable params: 6,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100,activation=\"sigmoid\",input_shape=(X.shape[1],)))\n",
    "model.add(Dense(50,activation=\"sigmoid\"))\n",
    "model.add(Dense(10,activation=\"sigmoid\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kompilacja, uczenie i predykcja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "514/514 [==============================] - 1s 1ms/step - loss: 0.6902 - acc: 0.5409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/3\n",
      "514/514 [==============================] - 0s 102us/step - loss: 0.6375 - acc: 0.6595\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "514/514 [==============================] - 0s 112us/step - loss: 0.6368 - acc: 0.6595\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=3)\n",
    "model.predict_classes(X_test)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacje w sieciach neuronowych\n",
    "\n",
    "1) Kara za wielkości wag (np. regularyzacja L2):\n",
    "\n",
    "$$Objective = Cost + \\alpha\\sum w_i^2$$\n",
    "\n",
    "\n",
    "2) Dropout:\n",
    "\n",
    "<img src=\"Grafika/dropout.jpeg\" width=\"500\">\n",
    "\n",
    "Źródło: http://cs231n.github.io/neural-networks-2/\n",
    "\n",
    "\n",
    "W praktyce zatrzymujemy uczenie sieci przy użyciu \"early stopping\" - oceniamy sieć po każdej epoce i przerywamy uczenie, jeśli model już się nie poprawia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5,activation=\"relu\",input_shape=(X_train.shape[1],),kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation=\"sigmoid\",kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "# zapiszemy model z najlepszej epoki:\n",
    "save_best_model = ModelCheckpoint(\"wagi_best.h5py\",save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train, \n",
    "          batch_size=32, \n",
    "          validation_split=0.3,\n",
    "          epochs=100,\n",
    "          callbacks=[early_stopping,save_best_model])\n",
    "\n",
    "\n",
    "# wczytujemy model z najlepszej epoki:\n",
    "model.load_weights(\"wagi_best.h5py\")\n",
    "model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
